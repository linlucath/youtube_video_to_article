import os
import asyncio
import aiohttp
import json
import time
import argparse
from typing import List, Dict, Any
import logging
import re
import sys
import glob
from pathlib import Path

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ä»¥æ”¯æŒä¸­æ–‡å­—ç¬¦
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

class OptimizedSubtitleConverter:
    def __init__(self, api_key: str, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–è½¬æ¢å™¨
        
        Args:
            api_key: DeepSeek APIå¯†é’¥
            config: é…ç½®å‚æ•°
        """
        self.api_key = api_key
        self.base_url = "https://api.deepseek.com/chat/completions"
        
        # ä¼˜åŒ–çš„é»˜è®¤é…ç½®
        default_config = {
            "model": "deepseek-chat",
            "temperature": 0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            "max_tokens": 3500,
            "chunk_size": 200,  # æ¯å—é»˜è®¤åŒ…å«çš„å•è¯æ•°
            "request_timeout": 40,
            "retry_attempts": 3,
            "retry_delay": 2,
            "enable_retry": True  # é»˜è®¤å¯ç”¨é‡å¤„ç†
        }
        
        self.config = {**default_config, **(config or {})}
        
        # å¤±è´¥å†…å®¹æ”¶é›†
        self.failed_chunks = []
        
        # è®¾ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        log_file = log_dir / f'converter_{time.strftime("%Y%m%d_%H%M%S")}.log'
        
        # åˆ›å»ºä¸“å±çš„loggerï¼Œé¿å…ä¸å…¶ä»–loggerå†²çª
        self.logger = logging.getLogger(f'OptimizedSubtitleConverter_{id(self)}')
        self.logger.setLevel(logging.DEBUG)  # è®¾ç½®ä¸ºDEBUGçº§åˆ«ä»¥æ•è·æ‰€æœ‰æ—¥å¿—
        
        # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ—§handlers
        self.logger.handlers.clear()
        
        # æ–‡ä»¶handler - ä½¿ç”¨utf-8-sigç¼–ç ä»¥ä¾¿Windowsè®°äº‹æœ¬èƒ½æ­£ç¡®æ˜¾ç¤º
        file_handler = logging.FileHandler(log_file, encoding='utf-8-sig', mode='w')
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)
        
        # æ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(console_formatter)
        
        # æ·»åŠ handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°root logger
        self.logger.propagate = False
        
        self.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        self.logger.debug(f"ğŸ”§ Loggeråˆå§‹åŒ–å®Œæˆï¼ŒID: {id(self)}")
        
        
        self.prompt_template = """ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ä¸“å®¶å’Œç¿»è¯‘ä¸“å®¶, è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å¤„ç†å­—å¹•æ–‡æœ¬ï¼š

==== å¤„ç†è¦æ±‚ ====
1. **é”™è¯¯ä¿®æ­£**ï¼šè¯†åˆ«å¹¶ä¿®æ­£å¯èƒ½å­˜åœ¨çš„è¯†åˆ«é”™è¯¯, ç¡®ä¿æ–‡æœ¬å‡†ç¡®æ€§
2. **å®Œæ•´æ€§æ£€æŸ¥**ï¼šæ£€æŸ¥æ–‡æœ¬æœ«å°¾ï¼Œè¯†åˆ«è¢«æˆªæ–­çš„ä¸å®Œæ•´å¥å­
3. **æ®µè½æ•´ç†**ï¼šä»…å¯¹å®Œæ•´å¥å­è¿›è¡Œæ®µè½é‡ç»„, å°†ä¸å®Œæ•´å¥å­åŸæ ·è¿”å›
4. **ç¿»è¯‘å‡†ç¡®æ€§**ï¼šæä¾›å‡†ç¡®ã€ä¸“ä¸šã€ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯çš„ç¿»è¯‘

==== è¾“å‡ºæ ¼å¼ ====
å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸å¾—æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š

[å»é™¤ä¸å®Œæ•´å¥å­çš„è‹±æ–‡æ•´ç†æ®µè½]

[å¯¹åº”ä¸­æ–‡ç¿»è¯‘]

[ä¸å®Œæ•´å¥å­: åŸå§‹ä¸å®Œæ•´æ–‡æœ¬]

==== ç¦æ­¢äº‹é¡¹ ====
- ç¦æ­¢æ·»åŠ ä»»ä½•è§£é‡Šè¯´æ˜
- ç¦æ­¢æ·»åŠ å¤„ç†æ­¥éª¤æè¿°  
- ç¦æ­¢ç¿»è¯‘ä¸å®Œæ•´å¥å­
- ç¦æ­¢è‡ªè¡Œè¡¥å…¨æˆªæ–­çš„å¥å­
- ç¦æ­¢æ·»åŠ æ ‡é¢˜ã€åºå·æˆ–å…¶ä»–æ ¼å¼åŒ–æ ‡è®°

åŸå§‹æ–‡æœ¬ï¼š
{chunk}

è¯·ç›´æ¥è¾“å‡ºå¤„ç†ç»“æœï¼š"""

    def read_file(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒå¤šç§ç¼–ç """
        encodings = ['utf-8', 'gbk', 'gb2312', 'ascii']
        
        self.logger.info(f"ğŸ“– å¼€å§‹è¯»å–æ–‡ä»¶: {file_path}")
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                self.logger.info(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼Œç¼–ç : {encoding}, é•¿åº¦: {len(content)}å­—ç¬¦")
                self.logger.debug(f"ğŸ“„ æ–‡ä»¶å†…å®¹é¢„è§ˆ:\n{content[:300]}...")
                return content
            except UnicodeDecodeError:
                self.logger.debug(f"âŒ ç¼–ç  {encoding} è¯»å–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                continue
        
        self.logger.error(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ï¼Œå°è¯•çš„æ‰€æœ‰ç¼–ç å‡å¤±è´¥: {encodings}")
        raise ValueError(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œå°è¯•çš„ç¼–ç : {encodings}")
    
    
    def strip_content(self, content: str) -> str:
        """æ¸…ç†AIè¿”å›çš„å†…å®¹"""
        return content.strip()

    async def process_chunk_async(self, session: aiohttp.ClientSession, chunk: str, chunk_index: int) -> tuple:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡æœ¬å—"""
        
        self.logger.debug(f"ğŸ“¤ å¼€å§‹å¤„ç†å— {chunk_index + 1}, è¾“å…¥é•¿åº¦: {len(chunk)}å­—ç¬¦")
        self.logger.debug(f"ğŸ“¤ è¾“å…¥å†…å®¹é¢„è§ˆ: '{chunk[:200]}...'")
        
        for attempt in range(self.config['retry_attempts']):
            try:
                prompt = self.prompt_template.format(chunk=chunk)
                self.logger.debug(f"ğŸ”§ ç”Ÿæˆçš„æç¤ºè¯é•¿åº¦: {len(prompt)}å­—ç¬¦")
                
                payload = {
                    "model": self.config['model'],
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸“ä¸šçš„åŒè¯­æ–‡æ¡£ç¼–è¾‘åŠ©æ‰‹ï¼Œä¸“æ³¨äºé«˜è´¨é‡çš„å†…å®¹æ•´ç†å’Œç¿»è¯‘ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": self.config['temperature'],
                    "max_tokens": self.config['max_tokens']
                }
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                timeout = aiohttp.ClientTimeout(total=self.config['request_timeout'])
                
                self.logger.debug(f"ğŸŒ å‘é€APIè¯·æ±‚åˆ° {self.base_url}")
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=timeout
                ) as response:
                    
                    self.logger.debug(f"ğŸ“¡ æ”¶åˆ°å“åº”ï¼ŒçŠ¶æ€ç : {response.status}")
                    
                    if response.status == 200:
                        result = await response.json()
                        if 'choices' in result and result['choices']:
                            raw_content = result['choices'][0]['message']['content'].strip()
                            self.logger.debug(f"ğŸ¤– AIåŸå§‹è¿”å›é•¿åº¦: {len(raw_content)}å­—ç¬¦")
                            self.logger.debug(f"ğŸ¤– AIè¿”å›å¼€å¤´: '{raw_content[:150]}...'")
                            
                            # æ¸…ç†å†…å®¹
                            content = self.strip_content(raw_content)
                            self.logger.debug(f"ğŸ§¹ æ¸…ç†åé•¿åº¦: {len(content)}å­—ç¬¦")
                            
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸å®Œæ•´å¥å­æ ‡è®°
                            if '[ä¸å®Œæ•´å¥å­:' in content:
                                self.logger.info(f"ğŸ”— AIæ£€æµ‹åˆ°ä¸å®Œæ•´å¥å­åœ¨å— {chunk_index + 1}")
                            else:
                                self.logger.debug(f"âœ… å— {chunk_index + 1} æ— ä¸å®Œæ•´å¥å­")
                            
                            self.logger.info(f"âœ… å®Œæˆå— {chunk_index + 1}")
                            return (chunk_index, content)
                        else:
                            raise Exception(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                    else:
                        error_text = await response.text()
                        self.logger.error(f"APIé”™è¯¯è¯¦æƒ… - çŠ¶æ€ç : {response.status}, å“åº”: {error_text}")
                        raise Exception(f"APIé”™è¯¯ {response.status}: {error_text}")
                        
            except Exception as e:
                wait_time = self.config['retry_delay'] * (2 ** attempt)
                self.logger.warning(f"å— {chunk_index + 1} å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {e}")
                
                if attempt < self.config['retry_attempts'] - 1:
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"å— {chunk_index + 1} å½»åº•å¤±è´¥ï¼Œä¿ç•™åŸå§‹å†…å®¹")
                    # è®°å½•å¤±è´¥çš„å—ä¿¡æ¯
                    failed_chunk_info = {
                        'index': chunk_index,
                        'content': chunk,
                        'error': str(e)
                    }
                    self.failed_chunks.append(failed_chunk_info)
                    return (chunk_index, f"# å¤„ç†å¤±è´¥çš„å†…å®¹\n\n{chunk}")

    def split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬ä¸ºå—"""
        chunk_size = self.config['chunk_size']
        words = text.split()
        self.logger.info(f"ğŸ“Š æ–‡æœ¬ç»Ÿè®¡: æ€»å­—ç¬¦æ•°={len(text)}, æ€»å•è¯æ•°={len(words)}")
        self.logger.info(f"ğŸ“Š åˆ†å—é…ç½®: æ¯å—{chunk_size}å•è¯")
        
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
            self.logger.debug(f"âœ‚ï¸  å— {len(chunks)}: {len(chunk)}å­—ç¬¦, {len(chunk.split())}å•è¯")

        self.logger.info(f"âœ‚ï¸  åˆ†å‰²å®Œæˆ: å…±{len(chunks)}å—")
        return chunks

    async def process_file_async(self, input_file: str, output_file: str = None):
        """å¼‚æ­¥é¡ºåºå¤„ç†æ–‡ä»¶"""
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {input_file}")
        
        if output_file is None:
            name, ext = os.path.splitext(input_file)
            output_file = f"{name}_optimized.md"
        
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†: {input_file}")
        self.logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # è¯»å–å’Œåˆ†å‰²æ–‡ä»¶
        content = self.read_file(input_file)
        chunks = self.split_text(content)
        total_chunks = len(chunks)
        
        self.logger.info(f"ğŸ“Š æ™ºèƒ½åˆ†å‰²ä¸º {total_chunks} å—")
        self.logger.info(f"ğŸ”„ ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼")
        
        start_time = time.time()
        
        # é¡ºåºå¤„ç†å¹¶å¤„ç†ä¸å®Œæ•´å¥å­
        processed_chunks = await self.process_chunks_sequentially(chunks)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„å—éœ€è¦é‡æ–°å¤„ç†
        if self.failed_chunks and self.config.get('enable_retry', False):
            self.logger.info(f"ğŸ”„ å‘ç° {len(self.failed_chunks)} ä¸ªå¤±è´¥çš„å—ï¼Œå¼€å§‹é‡æ–°å¤„ç†...")
            reprocessed_chunks = await self.reprocess_failed_chunks()
            
            # åˆ›å»ºå¤±è´¥å—ç´¢å¼•åˆ°é‡å¤„ç†ç»“æœçš„æ˜ å°„
            reprocess_map = {r['original_index']: r for r in reprocessed_chunks}
            
            # æ›¿æ¢å¤±è´¥çš„å†…å®¹
            for i, chunk in enumerate(processed_chunks):
                if chunk.startswith("# å¤„ç†å¤±è´¥çš„å†…å®¹"):
                    # æ ¹æ®ä½ç½®æ¨æ–­åŸå§‹å—ç´¢å¼•
                    if i in reprocess_map:
                        reprocessed = reprocess_map[i]
                        processed_chunks[i] = reprocessed['content']
                        status = "âœ… æˆåŠŸ" if reprocessed['success'] else "ğŸ”§ å·²æ¸…ç†"
                        self.logger.info(f"{status} æ›¿æ¢å— {i+1} çš„å¤±è´¥å†…å®¹")
            
            # æ¸…ç©ºå¤±è´¥åˆ—è¡¨
            self.failed_chunks.clear()
        elif self.failed_chunks:
            self.logger.info(f"âš ï¸  æœ‰ {len(self.failed_chunks)} ä¸ªå—å¤„ç†å¤±è´¥ï¼Œä½†æœªå¯ç”¨è‡ªåŠ¨é‡å¤„ç†")
        
        # åˆå¹¶å†…å®¹
        final_content = self.merge_content(processed_chunks)
        
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(final_content)
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ å¤„ç†å®Œæˆï¼")
        self.logger.info(f"â±ï¸  ç”¨æ—¶: {elapsed_time:.1f} ç§’")
        self.logger.info(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {total_chunks/elapsed_time:.1f} å—/ç§’")
        
        return output_file

    async def process_chunks_sequentially(self, chunks: List[str]) -> List[str]:
        """é¡ºåºå¤„ç†æ–‡æœ¬å—ï¼Œå¤„ç†ä¸å®Œæ•´å¥å­ä¼ é€’"""
        processed_chunks = []
        incomplete_sentence = ""
        
        connector = aiohttp.TCPConnector(limit=1, limit_per_host=1)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            for i, chunk in enumerate(chunks):
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†å— {i+1}/{len(chunks)}")
                
                original_chunk = chunk
                # å¦‚æœæœ‰ä¸å®Œæ•´å¥å­ï¼Œæ·»åŠ åˆ°å½“å‰å—å¼€å¤´
                if incomplete_sentence:
                    chunk = incomplete_sentence + " " + chunk
                    self.logger.info(f"ğŸ“ å— {i+1} åˆå¹¶äº†å‰ä¸€å—çš„ä¸å®Œæ•´å¥å­")
                    self.logger.debug(f"ğŸ”— ä¸å®Œæ•´å¥å­: '{incomplete_sentence[:100]}...'")
                    self.logger.debug(f"ğŸ“ åŸå§‹å—å¼€å¤´: '{original_chunk[:100]}...'")
                    self.logger.debug(f"ï¿½ åˆå¹¶åå—å¼€å¤´: '{chunk[:100]}...'")
                else:
                    self.logger.debug(f"ğŸ“ å½“å‰å—å¼€å¤´: '{chunk[:100]}...'")
                
                self.logger.info(f"ğŸ“Š å½“å‰å—ç»Ÿè®¡: {len(chunk)}å­—ç¬¦, {len(chunk.split())}å•è¯")
                
                # å¤„ç†å½“å‰å—
                try:
                    result = await self.process_chunk_async(session, chunk, i)
                    index, content = result

                    self.logger.info(f"ai è¿”å›å†…å®¹:\n{content}\n")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´å¥å­
                    content, new_incomplete = self.extract_incomplete_sentence(content)
                    
                    self.logger.debug(f"ğŸ“Š å¤„ç†åå†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
                    processed_chunks.append(content)
                    
                    # æ›´æ–°ä¸å®Œæ•´å¥å­çŠ¶æ€
                    if new_incomplete:
                        self.logger.info(f"ğŸ”— æå–åˆ°ä¸å®Œæ•´å¥å­: '{new_incomplete[:80]}...'")
                        self.logger.info(f"ï¿½ å°†ä¼ é€’ç»™å— {i+2}")
                        incomplete_sentence = new_incomplete
                    else:
                        if incomplete_sentence:
                            self.logger.info(f"âœ… å‰ä¸€ä¸ªä¸å®Œæ•´å¥å­å·²å¤„ç†å®Œæˆ")
                        incomplete_sentence = ""
                    
                except Exception as e:
                    self.logger.error(f"âŒ å— {i+1} å¤„ç†å¤±è´¥: {e}")
                    processed_chunks.append(f"# å¤„ç†å¤±è´¥çš„å†…å®¹\n\n{chunk}")
                    incomplete_sentence = ""  # é‡ç½®ä¸å®Œæ•´å¥å­
        
        # å¤„ç†æœ€åå¯èƒ½å‰©ä½™çš„ä¸å®Œæ•´å¥å­
        if incomplete_sentence:
            self.logger.warning(f"âš ï¸  æ–‡ä»¶æœ«å°¾å­˜åœ¨æœªå¤„ç†çš„ä¸å®Œæ•´å¥å­: {incomplete_sentence}")
            if processed_chunks:
                processed_chunks[-1] += f"\n\n[æ–‡ä»¶æœ«å°¾ä¸å®Œæ•´å¥å­: {incomplete_sentence}]"
        
        return processed_chunks
    
    def extract_incomplete_sentence(self, content: str) -> tuple[str, str]:
        """ä»å¤„ç†ç»“æœä¸­æå–ä¸å®Œæ•´å¥å­"""
        if not content:
            self.logger.debug("âš ï¸  å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¸å®Œæ•´å¥å­æ£€æŸ¥")
            return content, ""
        
        self.logger.debug(f"ğŸ” æ£€æŸ¥ä¸å®Œæ•´å¥å­æ ‡è®°ï¼Œå†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
        self.logger.debug(f"ğŸ” å†…å®¹é¢„è§ˆ: '{content[:300]}...'")
        
        # æŸ¥æ‰¾ä¸å®Œæ•´å¥å­æ ‡è®°
        incomplete_pattern = r'\[ä¸å®Œæ•´å¥å­:\s*([^\]]+)\]'
        match = re.search(incomplete_pattern, content)
        
        if match:
            incomplete_sentence = match.group(1).strip()
            self.logger.info(f"ğŸ”— å‘ç°ä¸å®Œæ•´å¥å­æ ‡è®°!")
            self.logger.info(f"ğŸ”— ä¸å®Œæ•´å¥å­å†…å®¹: '{incomplete_sentence[:150]}...'")
            self.logger.debug(f"ğŸ”— ä¸å®Œæ•´å¥å­é•¿åº¦: {len(incomplete_sentence)}å­—ç¬¦")
            
            # ä»å†…å®¹ä¸­ç§»é™¤ä¸å®Œæ•´å¥å­æ ‡è®°
            clean_content = re.sub(incomplete_pattern, '', content).strip()
            self.logger.debug(f"ğŸ§¹ ç§»é™¤æ ‡è®°å‰é•¿åº¦: {len(content)}å­—ç¬¦")
            self.logger.debug(f"ğŸ§¹ ç§»é™¤æ ‡è®°åé•¿åº¦: {len(clean_content)}å­—ç¬¦")
            self.logger.debug(f"ğŸ§¹ æ¸…ç†åå†…å®¹é¢„è§ˆ: '{clean_content[:200]}...'")
            return clean_content, incomplete_sentence
        else:
            self.logger.debug("âœ… æœªå‘ç°ä¸å®Œæ•´å¥å­æ ‡è®°")
            self.logger.debug(f"âœ… å®Œæ•´å†…å®¹é¢„è§ˆ: '{content[:200]}...'")
        
        return content, ""
    
    def merge_content(self, chunks: List[str]) -> str:
        """åˆå¹¶å†…å®¹"""
        self.logger.info(f"ğŸ”— å¼€å§‹åˆå¹¶å†…å®¹: æ€»å—æ•°={len(chunks)}")
        
        # è¿‡æ»¤ç©ºå†…å®¹
        valid_chunks = [chunk for chunk in chunks if chunk and chunk.strip()]
        self.logger.info(f"ğŸ”— æœ‰æ•ˆå—æ•°: {len(valid_chunks)}/{len(chunks)}")
        
        if len(valid_chunks) < len(chunks):
            empty_count = len(chunks) - len(valid_chunks)
            self.logger.warning(f"âš ï¸  å‘ç° {empty_count} ä¸ªç©ºå—å·²è¢«è¿‡æ»¤")
        
        # åˆå¹¶
        content = '\n\n'.join(valid_chunks)
        self.logger.info(f"ğŸ”— åˆå¹¶åæ€»é•¿åº¦: {len(content)}å­—ç¬¦")
        self.logger.debug(f"ğŸ”— åˆå¹¶åçš„å†…å®¹é¢„è§ˆ:\n{content[:500]}...")
        
        # æœ€ç»ˆæ¸…ç†
        self.logger.info("ğŸ§¹ å¼€å§‹æœ€ç»ˆæ¸…ç†...")
        content = self.final_cleanup(content)
        self.logger.info(f"ğŸ§¹ æ¸…ç†åæ€»é•¿åº¦: {len(content)}å­—ç¬¦")
        
        return content

    def final_cleanup(self, content: str) -> str:
        """æœ€ç»ˆå†…å®¹æ¸…ç†"""
        self.logger.info("ğŸ§¹ å¼€å§‹æœ€ç»ˆæ¸…ç†...")
        self.logger.info(f"ğŸ§¹ æ¸…ç†å‰å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        self.logger.debug(f"ğŸ§¹ æ¸…ç†å‰å†…å®¹å¼€å¤´:\n{content[:500]}...")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹å¹¶åˆ†ç±»å¤„ç†
        brackets_content = re.findall(r'\[[^\]]*\]', content)
        if brackets_content:
            self.logger.info(f"ğŸ” å‘ç°æ–¹æ‹¬å·å†…å®¹: {len(brackets_content)} ä¸ª")
            for i, bracket in enumerate(brackets_content[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
                self.logger.debug(f"  ğŸ“‹ æ–¹æ‹¬å· {i+1}: {bracket[:100]}...")
        else:
            self.logger.info("âœ… æœªå‘ç°æ–¹æ‹¬å·å†…å®¹")
        
        # åªç§»é™¤ç‰¹å®šçš„æ ¼å¼æ ‡è®°å’Œä¸å®Œæ•´å¥å­æ ‡è®°ï¼Œä½†ä¿ç•™å…¶ä»–å¯èƒ½æœ‰ç”¨çš„æ ‡è®°
        format_markers = [
            r'\[è‹±æ–‡æ•´ç†æ®µè½\]',
            r'\[å¯¹åº”ä¸­æ–‡ç¿»è¯‘\]', 
            r'\[å¦‚æœ‰ä¸å®Œæ•´å¥å­\]',
            r'\[ä¸å®Œæ•´å¥å­:[^\]]*\]',
            r'\[æ–‡ä»¶æœ«å°¾ä¸å®Œæ•´å¥å­:[^\]]*\]',
            r'\[ä¸å®Œæ•´å¥å­\]'
        ]
        
        self.logger.info(f"ğŸ”§ å‡†å¤‡ç§»é™¤ {len(format_markers)} ç§æ ¼å¼æ ‡è®°")
        
        # ç§»é™¤AIæ·»åŠ çš„æ ¼å¼è¯´æ˜æ ‡è®°
        total_removed = 0
        for pattern in format_markers:
            before_len = len(content)
            content = re.sub(pattern, '', content)
            after_len = len(content)
            if before_len != after_len:
                removed = before_len - after_len
                total_removed += removed
                self.logger.info(f"  âœ‚ï¸  ç§»é™¤æ ¼å¼æ ‡è®°: {pattern[:30]}..., å‡å°‘ {removed} å­—ç¬¦")
        
        if total_removed > 0:
            self.logger.info(f"ğŸ—‘ï¸  å…±ç§»é™¤æ ¼å¼æ ‡è®°: {total_removed} å­—ç¬¦")
        else:
            self.logger.info("âœ… æœªå‘ç°éœ€è¦ç§»é™¤çš„æ ¼å¼æ ‡è®°")
        
        # ç§»é™¤å¯èƒ½åŒ…å«é•¿æ–‡æœ¬çš„æ–¹æ‹¬å·å—ï¼ˆè¿™äº›é€šå¸¸æ˜¯AIé”™è¯¯è¿”å›çš„æ ¼å¼ï¼‰
        # ä½†è¦å°å¿ƒä¸è¦ç§»é™¤çœŸæ­£çš„å†…å®¹
        long_bracket_pattern = r'\[[^\]]{100,}\]'  # è¶…è¿‡100å­—ç¬¦çš„æ–¹æ‹¬å·å†…å®¹
        long_brackets = re.findall(long_bracket_pattern, content)
        if long_brackets:
            self.logger.warning(f"âš ï¸  å‘ç° {len(long_brackets)} ä¸ªé•¿æ–¹æ‹¬å·å—ï¼Œå¯èƒ½æ˜¯æ ¼å¼é”™è¯¯")
            for i, bracket in enumerate(long_brackets[:3]):
                self.logger.debug(f"  ğŸ“‹ é•¿æ–¹æ‹¬å· {i+1} ({len(bracket)}å­—ç¬¦): {bracket[:150]}...")
            before_len = len(content)
            content = re.sub(long_bracket_pattern, '', content)
            after_len = len(content)
            self.logger.info(f"  âœ‚ï¸  ç§»é™¤é•¿æ–¹æ‹¬å·å—ï¼Œå‡å°‘ {before_len - after_len} å­—ç¬¦")
        else:
            self.logger.info("âœ… æœªå‘ç°å¼‚å¸¸é•¿æ–¹æ‹¬å·å—")
        
        self.logger.info(f"ğŸ“Š ç§»é™¤æ ¼å¼æ ‡è®°åé•¿åº¦: {len(content)} å­—ç¬¦")
        
        # ç¡®ä¿æ ‡é¢˜æ ¼å¼æ­£ç¡®
        before_len = len(content)
        content = re.sub(r'\n(#{1,6}\s)', r'\n\n\1', content)
        if len(content) != before_len:
            self.logger.debug(f"ğŸ”§ æ ‡é¢˜æ ¼å¼è°ƒæ•´ï¼Œé•¿åº¦å˜åŒ–: {len(content) - before_len}")
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        before_len = len(content)
        content = re.sub(r'\n{3,}', '\n\n', content)
        if len(content) != before_len:
            self.logger.debug(f"ğŸ”§ æ¸…ç†å¤šä½™ç©ºè¡Œï¼Œå‡å°‘ {before_len - len(content)} å­—ç¬¦")
        
        # æ¸…ç†é¦–å°¾ç©ºè¡Œ
        before_len = len(content)
        content = content.strip()
        if len(content) != before_len:
            self.logger.debug(f"ğŸ”§ æ¸…ç†é¦–å°¾ç©ºè¡Œï¼Œå‡å°‘ {before_len - len(content)} å­—ç¬¦")
        
        self.logger.info(f"âœ… æœ€ç»ˆæ¸…ç†å®Œæˆï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
        self.logger.debug(f"âœ… æ¸…ç†åå†…å®¹å¼€å¤´:\n{content[:500]}...")
        
        return content

    def process_file(self, input_file: str, output_file: str = None):
        """åŒæ­¥æ¥å£"""
        return asyncio.run(self.process_file_async(input_file, output_file))

    def batch_process_folder(self, input_folder: str, output_folder: str = None, file_pattern: str = "*.txt"):
        """æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        input_path = Path(input_folder)
        if not input_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹: {input_folder}")
        
        if not input_path.is_dir():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {input_folder}")
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹
        if output_folder is None:
            output_folder = input_path.parent / "processed"
        output_path = Path(output_folder)
        output_path.mkdir(exist_ok=True)
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        pattern_path = input_path / file_pattern
        files = list(input_path.glob(file_pattern))
        
        if not files:
            self.logger.warning(f"åœ¨æ–‡ä»¶å¤¹ {input_folder} ä¸­æœªæ‰¾åˆ°åŒ¹é… {file_pattern} çš„æ–‡ä»¶")
            return []
        
        self.logger.info(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        processed_files = []
        total_start_time = time.time()
        
        for i, input_file in enumerate(files, 1):
            try:
                self.logger.info(f"\n{'='*50}")
                self.logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {input_file.name}")
                self.logger.info(f"{'='*50}")
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                # å°†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºæ›´ç®€æ´çš„æ ¼å¼
                clean_name = self.clean_filename(input_file.stem)
                output_file = output_path / f"{clean_name}.md"
                
                # å¤„ç†æ–‡ä»¶
                result_file = self.process_file(str(input_file), str(output_file))
                processed_files.append({
                    'input': str(input_file),
                    'output': result_file,
                    'status': 'success'
                })
                
                self.logger.info(f"âœ… å®Œæˆ: {input_file.name} -> {Path(result_file).name}")
                
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥ {input_file.name}: {e}")
                processed_files.append({
                    'input': str(input_file),
                    'output': None,
                    'status': 'failed',
                    'error': str(e)
                })
        
        total_elapsed = time.time() - total_start_time
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for f in processed_files if f['status'] == 'success')
        failed_count = len(processed_files) - success_count
        
        self.logger.info(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        self.logger.info(f"ğŸ“Š æ€»è®¡: {len(processed_files)} ä¸ªæ–‡ä»¶")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
        self.logger.info(f"âŒ å¤±è´¥: {failed_count} ä¸ª")
        self.logger.info(f"â±ï¸  æ€»ç”¨æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ")
        self.logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
        
        return processed_files

    def clean_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç”Ÿæˆæ›´ç®€æ´çš„è¾“å‡ºæ–‡ä»¶å"""
        # æå–è®²åº§ç¼–å·
        lecture_match = re.search(r'Lecture\s*(\d+)', filename, re.IGNORECASE)
        if lecture_match:
            lecture_num = lecture_match.group(1)
            return f"Lecture{lecture_num}_Notes"
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®²åº§ç¼–å·ï¼Œæ¸…ç†ç‰¹æ®Šå­—ç¬¦
        clean = re.sub(r'[\[\](){}]', '', filename)
        clean = re.sub(r'[^\w\s-]', '', clean)
        clean = re.sub(r'\s+', '_', clean.strip())
        clean = clean.replace('__', '_').strip('_')
        
        # é™åˆ¶é•¿åº¦
        if len(clean) > 50:
            clean = clean[:50].rstrip('_')
        
        return clean or "processed_file"

    async def batch_process_folder_async(self, input_folder: str, output_folder: str = None, file_pattern: str = "*.txt"):
        """å¼‚æ­¥æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹"""
        # è¿™ä¸ªæ–¹æ³•å¯ä»¥ç”¨äºæœªæ¥çš„ä¼˜åŒ–ï¼Œç°åœ¨å…ˆä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
        return self.batch_process_folder(input_folder, output_folder, file_pattern)


def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–å­—å¹•è½¬æ¢å™¨ - é¡ºåºå¤„ç†ã€æ™ºèƒ½åˆ†å—ã€æ®µè½çº§ç¿»è¯‘')
    parser.add_argument('--input_path', help='è¾“å…¥å­—å¹•æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„', default='./raw')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„', default="output.md")
    parser.add_argument('-k', '--api-key', help='DeepSeek APIå¯†é’¥')
    parser.add_argument('--chunk-size', type=int, help='æ¯å—åŒ…å«çš„å•è¯æ•°', default=200)
    parser.add_argument('--temperature', type=float, default=0.1, help='AIæ¸©åº¦å‚æ•°')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼ï¼Œå¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶')
    parser.add_argument('--pattern', default='*.txt', help='æ‰¹é‡æ¨¡å¼ä¸‹çš„æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *.txt)')
    parser.add_argument('--enable-retry', action='store_true', help='å¯ç”¨å¤±è´¥å†…å®¹è‡ªåŠ¨é‡å¤„ç†')
    
    args = parser.parse_args()
    
    # è·å–APIå¯†é’¥
    api_key = args.api_key or os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: è¯·æä¾›DeepSeek APIå¯†é’¥")
        print("ğŸ’¡ æ–¹æ³•1: ä½¿ç”¨ -k å‚æ•°")
        print("ğŸ’¡ æ–¹æ³•2: è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")
        return
    
    # é…ç½®
    config = {
        "chunk_size": args.chunk_size,
        "temperature": args.temperature,
        "enable_retry": args.enable_retry
    }
    
    try:
        converter = OptimizedSubtitleConverter(api_key, config)
        
        # åˆ¤æ–­æ˜¯æ‰¹é‡å¤„ç†è¿˜æ˜¯å•æ–‡ä»¶å¤„ç†
        input_path = Path(args.input_path)
    
        print("ğŸš€ å¯åŠ¨æ‰¹é‡å¤„ç†æ¨¡å¼")
        if not input_path.is_dir():
            print(f"âŒ é”™è¯¯: æ‰¹é‡æ¨¡å¼éœ€è¦æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä½†æä¾›çš„æ˜¯æ–‡ä»¶: {args.input_path}")
            return
        
        results = converter.batch_process_folder(
            str(input_path), 
            args.output
        )
        
        success_count = sum(1 for r in results if r['status'] == 'success')
        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸå¤„ç† {success_count}/{len(results)} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        logging.error(f"å¤„ç†å¤±è´¥: {e}", exc_info=True)


if __name__ == "__main__":
    main()