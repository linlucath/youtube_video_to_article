import os
import asyncio
import aiohttp
import json
import time
import argparse
from typing import List, Dict, Any
import logging
import re
import sys
import glob
from pathlib import Path

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ä»¥æ”¯æŒä¸­æ–‡å­—ç¬¦
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

class OptimizedSubtitleConverter:
    def __init__(self, api_key: str, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–è½¬æ¢å™¨
        
        Args:
            api_key: DeepSeek APIå¯†é’¥
            config: é…ç½®å‚æ•°
        """
        self.api_key = api_key
        self.base_url = "https://api.deepseek.com/chat/completions"
        
        # ä¼˜åŒ–çš„é»˜è®¤é…ç½®
        default_config = {
            "model": "deepseek-chat",
            "temperature": 0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            "max_tokens": 8192,
            "request_timeout": 600,
            "retry_attempts": 3,
            "retry_delay": 2
        }
        
        self.config = {**default_config, **(config or {})}
        
        # å¤±è´¥å†…å®¹æ”¶é›†
        self.failed_chunks = []
        
        # è®¾ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        log_file = log_dir / f'converter_{time.strftime("%Y%m%d_%H%M%S")}.log'
        
        # åˆ›å»ºä¸“å±çš„loggerï¼Œé¿å…ä¸å…¶ä»–loggerå†²çª
        self.logger = logging.getLogger(f'OptimizedSubtitleConverter_{id(self)}')
        self.logger.setLevel(logging.DEBUG)  # è®¾ç½®ä¸ºDEBUGçº§åˆ«ä»¥æ•è·æ‰€æœ‰æ—¥å¿—
        
        # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ—§handlers
        self.logger.handlers.clear()
        
        # æ–‡ä»¶handler - ä½¿ç”¨utf-8-sigç¼–ç ä»¥ä¾¿Windowsè®°äº‹æœ¬èƒ½æ­£ç¡®æ˜¾ç¤º
        file_handler = logging.FileHandler(log_file, encoding='utf-8-sig', mode='w')
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)
        
        # æ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(console_formatter)
        
        # æ·»åŠ handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°root logger
        self.logger.propagate = False
        
        self.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        self.logger.debug(f"ğŸ”§ Loggeråˆå§‹åŒ–å®Œæˆï¼ŒID: {id(self)}")
        
        
        self.prompt_template = """ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ä¸“å®¶å’Œç¿»è¯‘ä¸“å®¶, è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å¤„ç†å­—å¹•æ–‡æœ¬ï¼š

==== å¤„ç†è¦æ±‚ ====
1. **é”™è¯¯ä¿®æ­£**ï¼šè¯†åˆ«å¹¶ä¿®æ­£å¯èƒ½å­˜åœ¨çš„è¯†åˆ«é”™è¯¯, ç¡®ä¿æ–‡æœ¬å‡†ç¡®æ€§
2. **å®Œæ•´æ€§æ£€æŸ¥**ï¼šæ£€æŸ¥æ–‡æœ¬æœ«å°¾ï¼Œè¯†åˆ«è¢«æˆªæ–­çš„ä¸å®Œæ•´å¥å­
3. **è¯­ä¹‰åˆ†æ®µ**ï¼šæ ¹æ®å†…å®¹çš„è¯­ä¹‰ä¸»é¢˜è‡ªç„¶åˆ†æ®µï¼Œç¡®ä¿æ¯ä¸ªæ®µè½å›´ç»•ä¸€ä¸ªå®Œæ•´çš„ä¸»é¢˜æˆ–è®ºç‚¹, ç”¨ç©ºè¡Œåˆ†å‰²
4. **ç¿»è¯‘å‡†ç¡®æ€§**ï¼šæä¾›å‡†ç¡®ã€ä¸“ä¸šã€ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯çš„ç¿»è¯‘

==== è¾“å‡ºæ ¼å¼ ====
ä¸¥æ ¼æŒ‰ç…§ã€Œè‹±æ–‡æ®µè½ + ç©ºè¡Œ + ä¸­æ–‡ç¿»è¯‘ + ç©ºè¡Œã€çš„æ ¼å¼äº¤æ›¿è¾“å‡ºï¼š

English paragraph 1.

ä¸­æ–‡ç¿»è¯‘1ã€‚

English paragraph 2.

ä¸­æ–‡ç¿»è¯‘2ã€‚

[ä¸å®Œæ•´å¥å­: åŸå§‹ä¸å®Œæ•´æ–‡æœ¬]

==== ç¤ºä¾‹ ====
è¾“å…¥æ–‡æœ¬ï¼š
hello world this is a test we are learning something new and then we

æ­£ç¡®è¾“å‡ºï¼š
Hello world. This is a test. We are learning something new.

ä½ å¥½ä¸–ç•Œ, è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•. æˆ‘ä»¬æ­£åœ¨å­¦ä¹ æ–°ä¸œè¥¿. 

[ä¸å®Œæ•´å¥å­: And then we]

==== æ³¨æ„äº‹é¡¹ ====
- æ¯ä¸ªè‹±æ–‡æ®µè½åç´§è·Ÿå…¶å¯¹åº”çš„ä¸­æ–‡ç¿»è¯‘
- æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
- ç¦æ­¢æ·»åŠ ä»»ä½•è§£é‡Šè¯´æ˜ã€æ ‡é¢˜ã€åºå·
- ç¦æ­¢ç¿»è¯‘ä¸å®Œæ•´å¥å­
- ç¦æ­¢è‡ªè¡Œè¡¥å…¨æˆªæ–­çš„å¥å­
- å¯¹äºä¸å®Œæ•´å¥å­çš„è¯†åˆ«åº”é‡‡å–ä¿å®ˆç­–ç•¥

åŸå§‹æ–‡æœ¬ï¼š
{chunk}

è¯·ç›´æ¥è¾“å‡ºå¤„ç†ç»“æœï¼š"""

    def read_file(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒå¤šç§ç¼–ç """
        encodings = ['utf-8', 'gbk', 'gb2312', 'ascii']
        
        self.logger.info(f"ğŸ“– å¼€å§‹è¯»å–æ–‡ä»¶: {file_path}")
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                self.logger.info(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼Œç¼–ç : {encoding}, é•¿åº¦: {len(content)}å­—ç¬¦")
                self.logger.debug(f"ğŸ“„ æ–‡ä»¶å†…å®¹é¢„è§ˆ:\n{content[:300]}...")
                return content
            except UnicodeDecodeError:
                self.logger.debug(f"âŒ ç¼–ç  {encoding} è¯»å–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                continue
        
        self.logger.error(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ï¼Œå°è¯•çš„æ‰€æœ‰ç¼–ç å‡å¤±è´¥: {encodings}")
        raise ValueError(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œå°è¯•çš„ç¼–ç : {encodings}")
    
    
    def strip_content(self, content: str) -> str:
        """æ¸…ç†AIè¿”å›çš„å†…å®¹"""
        return content.strip()

    async def process_chunk_async(self, session: aiohttp.ClientSession, chunk: str, chunk_index: int) -> tuple:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡æœ¬å—"""
        
        self.logger.debug(f"ğŸ“¤ å¼€å§‹å¤„ç†å— {chunk_index + 1}, è¾“å…¥é•¿åº¦: {len(chunk)}å­—ç¬¦")
        self.logger.debug(f"ğŸ“¤ è¾“å…¥å†…å®¹é¢„è§ˆ: '{chunk[:200]}...'")
        
        for attempt in range(self.config['retry_attempts']):
            try:
                prompt = self.prompt_template.format(chunk=chunk)
                self.logger.debug(f"ğŸ”§ ç”Ÿæˆçš„æç¤ºè¯é•¿åº¦: {len(prompt)}å­—ç¬¦")
                self.logger.debug(f"ğŸ“¤ å‘é€ç»™AIçš„å®Œæ•´å†…å®¹:\n{'='*80}\n{prompt}\n{'='*80}")
                
                payload = {
                    "model": self.config['model'],
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸“ä¸šçš„åŒè¯­æ–‡æ¡£ç¼–è¾‘åŠ©æ‰‹ï¼Œç¿»è¯‘ä¸“å®¶, ä¸“æ³¨äºé«˜è´¨é‡çš„å†…å®¹æ•´ç†å’Œç¿»è¯‘ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": self.config['temperature'],
                    "max_tokens": self.config['max_tokens']
                }
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                timeout = aiohttp.ClientTimeout(total=self.config['request_timeout'])
                
                self.logger.debug(f"ğŸŒ å‘é€APIè¯·æ±‚åˆ° {self.base_url}")
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=timeout
                ) as response:
                    
                    self.logger.debug(f"ğŸ“¡ æ”¶åˆ°å“åº”ï¼ŒçŠ¶æ€ç : {response.status}")
                    
                    if response.status == 200:
                        result = await response.json()
                        if 'choices' in result and result['choices']:
                            raw_content = result['choices'][0]['message']['content'].strip()
                            self.logger.debug(f"ğŸ¤– AIåŸå§‹è¿”å›é•¿åº¦: {len(raw_content)}å­—ç¬¦")
                            self.logger.debug(f"ğŸ¤– AIè¿”å›çš„å®Œæ•´å†…å®¹:\n{'='*80}\n{raw_content}\n{'='*80}")
                            
                            # æ¸…ç†å†…å®¹
                            content = self.strip_content(raw_content)
                            self.logger.debug(f"ğŸ§¹ æ¸…ç†åé•¿åº¦: {len(content)}å­—ç¬¦")
                            
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸å®Œæ•´å¥å­æ ‡è®°
                            if '[ä¸å®Œæ•´å¥å­:' in content:
                                self.logger.info(f"ğŸ”— AIæ£€æµ‹åˆ°ä¸å®Œæ•´å¥å­åœ¨å— {chunk_index + 1}")
                            else:
                                self.logger.debug(f"âœ… å— {chunk_index + 1} æ— ä¸å®Œæ•´å¥å­")
                            
                            self.logger.info(f"âœ… å®Œæˆå— {chunk_index + 1}")
                            return (chunk_index, content)
                        else:
                            raise Exception(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                    else:
                        error_text = await response.text()
                        self.logger.error(f"APIé”™è¯¯è¯¦æƒ… - çŠ¶æ€ç : {response.status}, å“åº”: {error_text}")
                        raise Exception(f"APIé”™è¯¯ {response.status}: {error_text}")
                        
            except Exception as e:
                wait_time = self.config['retry_delay'] * (2 ** attempt)
                self.logger.warning(f"å— {chunk_index + 1} å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {e}")
                
                if attempt < self.config['retry_attempts'] - 1:
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"å— {chunk_index + 1} å½»åº•å¤±è´¥ï¼Œä¿ç•™åŸå§‹å†…å®¹")
                    # è®°å½•å¤±è´¥çš„å—ä¿¡æ¯
                    failed_chunk_info = {
                        'index': chunk_index,
                        'content': chunk,
                        'error': str(e)
                    }
                    self.failed_chunks.append(failed_chunk_info)
                    return (chunk_index, f"# å¤„ç†å¤±è´¥çš„å†…å®¹\n\n{chunk}")

    def split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬ä¸ºå—"""
        chunk_size = self.config['chunk_size']
        words = text.split()
        self.logger.info(f"ğŸ“Š æ–‡æœ¬ç»Ÿè®¡: æ€»å­—ç¬¦æ•°={len(text)}, æ€»å•è¯æ•°={len(words)}")
        self.logger.info(f"ğŸ“Š åˆ†å—é…ç½®: æ¯å—{chunk_size}å•è¯")
        
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
            self.logger.debug(f"âœ‚ï¸  å— {len(chunks)}: {len(chunk)}å­—ç¬¦, {len(chunk.split())}å•è¯")

        self.logger.info(f"âœ‚ï¸  åˆ†å‰²å®Œæˆ: å…±{len(chunks)}å—")
        return chunks

    async def process_file_async(self, input_file: str, output_file: str = None):
        """å¼‚æ­¥é¡ºåºå¤„ç†æ–‡ä»¶"""
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {input_file}")
        
        if output_file is None:
            name, ext = os.path.splitext(input_file)
            output_file = f"{name}_optimized.md"
        
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†: {input_file}")
        self.logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # è¯»å–å’Œåˆ†å‰²æ–‡ä»¶
        content = self.read_file(input_file)
        chunks = self.split_text(content)
        total_chunks = len(chunks)
        
        self.logger.info(f"ğŸ“Š æ™ºèƒ½åˆ†å‰²ä¸º {total_chunks} å—")
        self.logger.info(f"ğŸ”„ ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼")
        
        start_time = time.time()
        
        # é¡ºåºå¤„ç†å¹¶å¤„ç†ä¸å®Œæ•´å¥å­
        processed_chunks = await self.process_chunks_sequentially(chunks)
        
        # è®°å½•å¤±è´¥çš„å—
        if self.failed_chunks:
            self.logger.warning(f"âš ï¸  æœ‰ {len(self.failed_chunks)} ä¸ªå—å¤„ç†å¤±è´¥")
        
        # åˆå¹¶å†…å®¹å¹¶è·å–åˆ†å‰²ä½ç½®
        final_content, split_positions = self.merge_content(processed_chunks)
        
        # è¾¹ç•Œä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get('enable_boundary_optimization', False) and split_positions:
            self.logger.info(f"ğŸ”§ å¼€å§‹è¾¹ç•Œä¼˜åŒ–ï¼Œå…± {len(split_positions)} ä¸ªåˆ†å‰²ç‚¹...")
            try:
                final_content = await self.optimize_boundaries_async(final_content, split_positions)
                self.logger.info("âœ… è¾¹ç•Œä¼˜åŒ–å®Œæˆ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ è¾¹ç•Œä¼˜åŒ–å¤±è´¥: {e}")
        
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(final_content)
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ å¤„ç†å®Œæˆï¼")
        self.logger.info(f"â±ï¸  ç”¨æ—¶: {elapsed_time:.1f} ç§’")
        self.logger.info(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {total_chunks/elapsed_time:.1f} å—/ç§’")
        
        return output_file

    async def process_chunks_sequentially(self, chunks: List[str]) -> List[str]:
        """é¡ºåºå¤„ç†æ–‡æœ¬å—ï¼Œå¤„ç†ä¸å®Œæ•´å¥å­ä¼ é€’"""
        processed_chunks = []
        incomplete_sentence = ""
        
        connector = aiohttp.TCPConnector(limit=1, limit_per_host=1)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            for i, chunk in enumerate(chunks):
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†å— {i+1}/{len(chunks)}")
                
                original_chunk = chunk
                # å¦‚æœæœ‰ä¸å®Œæ•´å¥å­ï¼Œæ·»åŠ åˆ°å½“å‰å—å¼€å¤´
                if incomplete_sentence:
                    chunk = incomplete_sentence + " " + chunk
                    self.logger.info(f"ğŸ“ å— {i+1} åˆå¹¶äº†å‰ä¸€å—çš„ä¸å®Œæ•´å¥å­")
                    self.logger.debug(f"ğŸ”— ä¸å®Œæ•´å¥å­: '{incomplete_sentence[:100]}...'")
                    self.logger.debug(f"ğŸ“ åŸå§‹å—å¼€å¤´: '{original_chunk[:100]}...'")
                    self.logger.debug(f"ï¿½ åˆå¹¶åå—å¼€å¤´: '{chunk[:100]}...'")
                else:
                    self.logger.debug(f"ğŸ“ å½“å‰å—å¼€å¤´: '{chunk[:100]}...'")
                
                self.logger.info(f"ğŸ“Š å½“å‰å—ç»Ÿè®¡: {len(chunk)}å­—ç¬¦, {len(chunk.split())}å•è¯")
                
                # å¤„ç†å½“å‰å—
                try:
                    result = await self.process_chunk_async(session, chunk, i)
                    index, content = result

                    self.logger.info(f"âœ… å— {i+1} å¤„ç†æˆåŠŸï¼Œè¿”å›å†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´å¥å­
                    content, new_incomplete = self.extract_incomplete_sentence(content)
                    
                    self.logger.debug(f"ğŸ“Š å¤„ç†åå†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
                    processed_chunks.append(content)
                    
                    # æ›´æ–°ä¸å®Œæ•´å¥å­çŠ¶æ€
                    if new_incomplete:
                        self.logger.info(f"ğŸ”— æå–åˆ°ä¸å®Œæ•´å¥å­: '{new_incomplete[:80]}...'")
                        self.logger.info(f"ï¿½ å°†ä¼ é€’ç»™å— {i+2}")
                        incomplete_sentence = new_incomplete
                    else:
                        if incomplete_sentence:
                            self.logger.info(f"âœ… å‰ä¸€ä¸ªä¸å®Œæ•´å¥å­å·²å¤„ç†å®Œæˆ")
                        incomplete_sentence = ""
                    
                except Exception as e:
                    self.logger.error(f"âŒ å— {i+1} å¤„ç†å¤±è´¥: {e}")
                    processed_chunks.append(f"# å¤„ç†å¤±è´¥çš„å†…å®¹\n\n{chunk}")
                    incomplete_sentence = ""  # é‡ç½®ä¸å®Œæ•´å¥å­
        
        # å¤„ç†æœ€åå¯èƒ½å‰©ä½™çš„ä¸å®Œæ•´å¥å­
        if incomplete_sentence:
            self.logger.warning(f"âš ï¸  æ–‡ä»¶æœ«å°¾å­˜åœ¨æœªå¤„ç†çš„ä¸å®Œæ•´å¥å­: {incomplete_sentence}")
            if processed_chunks:
                processed_chunks[-1] += f"\n\n[æ–‡ä»¶æœ«å°¾ä¸å®Œæ•´å¥å­: {incomplete_sentence}]"
        
        return processed_chunks
    
    def extract_incomplete_sentence(self, content: str) -> tuple[str, str]:
        """ä»å¤„ç†ç»“æœä¸­æå–ä¸å®Œæ•´å¥å­"""
        if not content:
            self.logger.debug("âš ï¸  å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¸å®Œæ•´å¥å­æ£€æŸ¥")
            return content, ""
        
        self.logger.debug(f"ğŸ” æ£€æŸ¥ä¸å®Œæ•´å¥å­æ ‡è®°ï¼Œå†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
        self.logger.debug(f"ğŸ” å†…å®¹é¢„è§ˆ: '{content[:300]}...'")
        
        # æŸ¥æ‰¾ä¸å®Œæ•´å¥å­æ ‡è®°
        incomplete_pattern = r'\[ä¸å®Œæ•´å¥å­:\s*([^\]]+)\]'
        match = re.search(incomplete_pattern, content)
        
        if match:
            incomplete_sentence = match.group(1).strip()
            self.logger.info(f"ğŸ”— å‘ç°ä¸å®Œæ•´å¥å­æ ‡è®°!")
            self.logger.info(f"ğŸ”— ä¸å®Œæ•´å¥å­å†…å®¹: '{incomplete_sentence[:150]}...'")
            self.logger.debug(f"ğŸ”— ä¸å®Œæ•´å¥å­é•¿åº¦: {len(incomplete_sentence)}å­—ç¬¦")
            
            # ä»å†…å®¹ä¸­ç§»é™¤ä¸å®Œæ•´å¥å­æ ‡è®°
            clean_content = re.sub(incomplete_pattern, '', content).strip()
            self.logger.debug(f"ğŸ§¹ ç§»é™¤æ ‡è®°å‰é•¿åº¦: {len(content)}å­—ç¬¦")
            self.logger.debug(f"ğŸ§¹ ç§»é™¤æ ‡è®°åé•¿åº¦: {len(clean_content)}å­—ç¬¦")
            self.logger.debug(f"ğŸ§¹ æ¸…ç†åå†…å®¹é¢„è§ˆ: '{clean_content[:200]}...'")
            return clean_content, incomplete_sentence
        else:
            self.logger.debug("âœ… æœªå‘ç°ä¸å®Œæ•´å¥å­æ ‡è®°")
        
        return content, ""
    
    def merge_content(self, chunks: List[str]) -> tuple:
        """
        åˆå¹¶å†…å®¹å¹¶è®°å½•åˆ†å‰²ä½ç½®
        
        Returns:
            (åˆå¹¶åçš„å†…å®¹, åˆ†å‰²ä½ç½®åˆ—è¡¨)
        """
        self.logger.info(f"ğŸ”— å¼€å§‹åˆå¹¶å†…å®¹: æ€»å—æ•°={len(chunks)}")
        
        # è¿‡æ»¤ç©ºå†…å®¹
        valid_chunks = [chunk for chunk in chunks if chunk and chunk.strip()]
        self.logger.info(f"ğŸ”— æœ‰æ•ˆå—æ•°: {len(valid_chunks)}/{len(chunks)}")
        
        if len(valid_chunks) < len(chunks):
            empty_count = len(chunks) - len(valid_chunks)
            self.logger.warning(f"âš ï¸  å‘ç° {empty_count} ä¸ªç©ºå—å·²è¢«è¿‡æ»¤")
        
        # åˆå¹¶å¹¶è®°å½•åˆ†å‰²ä½ç½®
        split_positions = []
        content = ""
        for i, chunk in enumerate(valid_chunks):
            if i > 0:
                # è®°å½•åˆ†å‰²ç‚¹ä½ç½®ï¼ˆåœ¨æ·»åŠ åˆ†éš”ç¬¦ä¹‹å‰ï¼‰
                split_positions.append(len(content))
                content += "\n\n"
            content += chunk
        
        self.logger.info(f"ğŸ”— åˆå¹¶åæ€»é•¿åº¦: {len(content)}å­—ç¬¦")
        self.logger.info(f"ğŸ”— è®°å½•äº† {len(split_positions)} ä¸ªåˆ†å‰²ä½ç½®")
        self.logger.debug(f"ğŸ”— åˆ†å‰²ä½ç½®: {split_positions[:10]}..." if len(split_positions) > 10 else f"ğŸ”— åˆ†å‰²ä½ç½®: {split_positions}")
        
        # æœ€ç»ˆæ¸…ç†
        self.logger.info("ğŸ§¹ å¼€å§‹æœ€ç»ˆæ¸…ç†...")
        content = self.final_cleanup(content)
        self.logger.info(f"ğŸ§¹ æ¸…ç†åæ€»é•¿åº¦: {len(content)}å­—ç¬¦")
        
        return content, split_positions

    def final_cleanup(self, content: str) -> str:
        """æœ€ç»ˆå†…å®¹æ¸…ç†"""
        self.logger.info("ğŸ§¹ å¼€å§‹æœ€ç»ˆæ¸…ç†...")
        self.logger.info(f"ğŸ§¹ æ¸…ç†å‰å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        self.logger.debug(f"ğŸ§¹ æ¸…ç†å‰å†…å®¹å¼€å¤´:\n{content[:500]}...")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹å¹¶åˆ†ç±»å¤„ç†
        brackets_content = re.findall(r'\[[^\]]*\]', content)
        if brackets_content:
            self.logger.info(f"ğŸ” å‘ç°æ–¹æ‹¬å·å†…å®¹: {len(brackets_content)} ä¸ª")
            for i, bracket in enumerate(brackets_content[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
                self.logger.debug(f"  ğŸ“‹ æ–¹æ‹¬å· {i+1}: {bracket[:100]}...")
        else:
            self.logger.info("âœ… æœªå‘ç°æ–¹æ‹¬å·å†…å®¹")
        
        # åªç§»é™¤ç‰¹å®šçš„æ ¼å¼æ ‡è®°å’Œä¸å®Œæ•´å¥å­æ ‡è®°ï¼Œä½†ä¿ç•™å…¶ä»–å¯èƒ½æœ‰ç”¨çš„æ ‡è®°
        format_markers = [
            r'\[è‹±æ–‡æ•´ç†æ®µè½\]',
            r'\[å¯¹åº”ä¸­æ–‡ç¿»è¯‘\]', 
            r'\[å¦‚æœ‰ä¸å®Œæ•´å¥å­\]',
            r'\[ä¸å®Œæ•´å¥å­:[^\]]*\]',
            r'\[æ–‡ä»¶æœ«å°¾ä¸å®Œæ•´å¥å­:[^\]]*\]',
            r'\[ä¸å®Œæ•´å¥å­\]'
        ]
        
        self.logger.info(f"ğŸ”§ å‡†å¤‡ç§»é™¤ {len(format_markers)} ç§æ ¼å¼æ ‡è®°")
        
        # ç§»é™¤AIæ·»åŠ çš„æ ¼å¼è¯´æ˜æ ‡è®°
        total_removed = 0
        for pattern in format_markers:
            before_len = len(content)
            content = re.sub(pattern, '', content)
            after_len = len(content)
            if before_len != after_len:
                removed = before_len - after_len
                total_removed += removed
                self.logger.info(f"  âœ‚ï¸  ç§»é™¤æ ¼å¼æ ‡è®°: {pattern[:30]}..., å‡å°‘ {removed} å­—ç¬¦")
        
        if total_removed > 0:
            self.logger.info(f"ğŸ—‘ï¸  å…±ç§»é™¤æ ¼å¼æ ‡è®°: {total_removed} å­—ç¬¦")
        else:
            self.logger.info("âœ… æœªå‘ç°éœ€è¦ç§»é™¤çš„æ ¼å¼æ ‡è®°")
        
        # ç§»é™¤å¯èƒ½åŒ…å«é•¿æ–‡æœ¬çš„æ–¹æ‹¬å·å—ï¼ˆè¿™äº›é€šå¸¸æ˜¯AIé”™è¯¯è¿”å›çš„æ ¼å¼ï¼‰
        # ä½†è¦å°å¿ƒä¸è¦ç§»é™¤çœŸæ­£çš„å†…å®¹
        long_bracket_pattern = r'\[[^\]]{100,}\]'  # è¶…è¿‡100å­—ç¬¦çš„æ–¹æ‹¬å·å†…å®¹
        long_brackets = re.findall(long_bracket_pattern, content)
        if long_brackets:
            self.logger.warning(f"âš ï¸  å‘ç° {len(long_brackets)} ä¸ªé•¿æ–¹æ‹¬å·å—ï¼Œå¯èƒ½æ˜¯æ ¼å¼é”™è¯¯")
            for i, bracket in enumerate(long_brackets[:3]):
                self.logger.debug(f"  ğŸ“‹ é•¿æ–¹æ‹¬å· {i+1} ({len(bracket)}å­—ç¬¦): {bracket[:150]}...")
            before_len = len(content)
            content = re.sub(long_bracket_pattern, '', content)
            after_len = len(content)
            self.logger.info(f"  âœ‚ï¸  ç§»é™¤é•¿æ–¹æ‹¬å·å—ï¼Œå‡å°‘ {before_len - after_len} å­—ç¬¦")
        else:
            self.logger.info("âœ… æœªå‘ç°å¼‚å¸¸é•¿æ–¹æ‹¬å·å—")
        
        self.logger.info(f"ğŸ“Š ç§»é™¤æ ¼å¼æ ‡è®°åé•¿åº¦: {len(content)} å­—ç¬¦")
        
        # ç¡®ä¿æ ‡é¢˜æ ¼å¼æ­£ç¡®
        before_len = len(content)
        content = re.sub(r'\n(#{1,6}\s)', r'\n\n\1', content)
        if len(content) != before_len:
            self.logger.debug(f"ğŸ”§ æ ‡é¢˜æ ¼å¼è°ƒæ•´ï¼Œé•¿åº¦å˜åŒ–: {len(content) - before_len}")
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        before_len = len(content)
        content = re.sub(r'\n{3,}', '\n\n', content)
        if len(content) != before_len:
            self.logger.debug(f"ğŸ”§ æ¸…ç†å¤šä½™ç©ºè¡Œï¼Œå‡å°‘ {before_len - len(content)} å­—ç¬¦")
        
        # æ¸…ç†é¦–å°¾ç©ºè¡Œ
        before_len = len(content)
        content = content.strip()
        if len(content) != before_len:
            self.logger.debug(f"ğŸ”§ æ¸…ç†é¦–å°¾ç©ºè¡Œï¼Œå‡å°‘ {before_len - len(content)} å­—ç¬¦")
        
        self.logger.info(f"âœ… æœ€ç»ˆæ¸…ç†å®Œæˆï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
        self.logger.debug(f"âœ… æ¸…ç†åå†…å®¹å¼€å¤´:\n{content[:500]}...")
        
        return content

    # ==================== è¾¹ç•Œä¼˜åŒ–ç›¸å…³æ–¹æ³• ====================
    
    def get_boundary_optimization_prompt(self) -> str:
        """è·å–è¾¹ç•Œä¼˜åŒ–çš„æç¤ºè¯"""
        return """ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ä¸“å®¶ã€‚ä»¥ä¸‹æ–‡æœ¬æ¥è‡ªè§†é¢‘å­—å¹•çš„åˆ†å—ç¿»è¯‘ï¼Œç”±äºåˆ†å—è¾¹ç•Œé—®é¢˜äº§ç”Ÿäº†**å†…å®¹é‡å¤**ã€‚

==== æ ¸å¿ƒé—®é¢˜ ====
åˆ†å—å¤„ç†æ—¶ï¼Œä¸€ä¸ªå¥å­å¯èƒ½è¢«æˆªæ–­ï¼š
- å‰ä¸€å—ç¿»è¯‘äº†å¥å­çš„ä¸€éƒ¨åˆ†ï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰
- åä¸€å—åˆä»å¤´ç¿»è¯‘äº†åŒä¸€å¥è¯

è¿™å¯¼è‡´**åŒä¸€ä¸ªæ„æ€è¢«ç¿»è¯‘äº†ä¸¤æ¬¡**ï¼Œäº§ç”Ÿé‡å¤çš„ä¸­æ–‡æ®µè½ã€‚

==== å…¸å‹é‡å¤æ¨¡å¼ ====
è¾“å…¥å¯èƒ½æ˜¯è¿™æ ·çš„ï¼š
```
We talked about loss functions to quantify how happy or unhappy.
æˆ‘ä»¬è®¨è®ºäº†ç”¨æŸå¤±å‡½æ•°æ¥é‡åŒ–æ»¡æ„æˆ–ä¸æ»¡æ„ç¨‹åº¦ã€‚

How happy or unhappy we are with different settings of the weights.
æˆ‘ä»¬å¯¹ä¸åŒæƒé‡è®¾ç½®çš„æ»¡æ„æˆ–ä¸æ»¡æ„ç¨‹åº¦ã€‚
```
è¿™é‡Œ"æ»¡æ„æˆ–ä¸æ»¡æ„ç¨‹åº¦"çš„æ„æ€é‡å¤äº†ã€‚

æ­£ç¡®è¾“å‡ºåº”è¯¥åˆå¹¶ä¸ºï¼š
```
We talked about loss functions to quantify how happy or unhappy we are with different settings of the weights.
æˆ‘ä»¬è®¨è®ºäº†ç”¨æŸå¤±å‡½æ•°æ¥é‡åŒ–æˆ‘ä»¬å¯¹ä¸åŒæƒé‡è®¾ç½®çš„æ»¡æ„æˆ–ä¸æ»¡æ„ç¨‹åº¦ã€‚
```

==== ä½ çš„ä»»åŠ¡ ====
1. **è¯†åˆ«è¯­ä¹‰é‡å¤**ï¼šæ‰¾å‡ºè¡¨è¾¾ç›¸åŒæˆ–ç›¸ä¼¼æ„æ€çš„æ®µè½
2. **åˆå¹¶é‡å¤å†…å®¹**ï¼šå°†é‡å¤çš„è‹±æ–‡å¥å­åˆå¹¶æˆå®Œæ•´å¥å­ï¼Œå¯¹åº”ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ä¸­æ–‡ç¿»è¯‘
3. **åˆ é™¤å†—ä½™**ï¼šåˆ é™¤å¤šä½™çš„ç¿»è¯‘ï¼Œä¿è¯æ¯ä¸ªæ„æ€åªå‡ºç°ä¸€æ¬¡
4. **ä¿æŒæ ¼å¼**ï¼šè‹±æ–‡æ®µè½ + ç©ºè¡Œ + ä¸­æ–‡ç¿»è¯‘ + ç©ºè¡Œ

==== è¾“å…¥å†…å®¹ ====
{boundary_content}

==== è¾“å‡ºè¦æ±‚ ====
ç›´æ¥è¾“å‡ºå»é‡åˆå¹¶åçš„å†…å®¹ã€‚æ ¼å¼ï¼š

è‹±æ–‡æ®µè½

ä¸­æ–‡ç¿»è¯‘

ï¼ˆä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ ‡è®°ï¼‰"""

    def extract_boundary_context(self, content: str, split_position: int) -> tuple:
        """
        æ ¹æ®åˆ†å‰²ä½ç½®æå–è¾¹ç•Œä¸Šä¸‹æ–‡ï¼ˆæå–åˆ†å‰²ç‚¹å‰åå„2ä¸ªæ®µè½ï¼‰
        
        Args:
            content: å®Œæ•´æ–‡æ¡£å†…å®¹
            split_position: åˆ†å‰²ç‚¹åœ¨æ–‡æ¡£ä¸­çš„å­—ç¬¦ä½ç½®
            
        Returns:
            (è¾¹ç•Œä¸Šä¸‹æ–‡å­—ç¬¦ä¸², å®é™…å¼€å§‹ä½ç½®, å®é™…ç»“æŸä½ç½®)
        """
        # å°†å†…å®¹æŒ‰åŒæ¢è¡Œåˆ†å‰²æˆæ®µè½
        parts = re.split(r'\n\n+', content)
        
        # è®¡ç®—æ¯ä¸ªæ®µè½åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        current_pos = 0
        split_para_idx = -1
        para_positions = []  # (start, end, content)
        
        for i, para in enumerate(parts):
            para_start = content.find(para, current_pos)
            if para_start == -1:
                para_start = current_pos
            para_end = para_start + len(para)
            para_positions.append((para_start, para_end, para))
            
            # æ‰¾åˆ°åˆ†å‰²ç‚¹æ‰€åœ¨æˆ–ä¹‹åçš„ç¬¬ä¸€ä¸ªæ®µè½
            if split_para_idx == -1 and para_end >= split_position:
                split_para_idx = i
            
            current_pos = para_end
        
        if split_para_idx == -1:
            split_para_idx = len(para_positions) - 1
        
        # æå–åˆ†å‰²ç‚¹å‰1ä¸ªæ®µè½å’Œå2ä¸ªæ®µè½ï¼ˆå…±4ä¸ªæ®µè½ï¼Œç¡®ä¿å®Œæ•´çš„è‹±ä¸­é…å¯¹ï¼‰
        # split_para_idx é€šå¸¸æ˜¯ chunk æœ«å°¾çš„ä¸­æ–‡æ®µè½ï¼Œæ‰€ä»¥ï¼š
        # - split_para_idx - 1: å¯¹åº”çš„è‹±æ–‡æ®µè½
        # - split_para_idx: ä¸­æ–‡æ®µè½ï¼ˆchunk1æœ«å°¾ï¼‰
        # - split_para_idx + 1: è‹±æ–‡æ®µè½ï¼ˆchunk2å¼€å¤´ï¼‰
        # - split_para_idx + 2: å¯¹åº”çš„ä¸­æ–‡æ®µè½
        start_idx = max(0, split_para_idx - 1)
        end_idx = min(len(para_positions), split_para_idx + 3)
        
        # è®¡ç®—å®é™…çš„å­—ç¬¦ä½ç½®
        start_pos = para_positions[start_idx][0]
        end_pos = para_positions[end_idx - 1][1]
        
        # æå–å†…å®¹
        boundary_content = content[start_pos:end_pos]
        
        self.logger.debug(f"ğŸ“ è¾¹ç•Œæå–: æ®µè½ {start_idx+1}-{end_idx}/{len(para_positions)}, å­—ç¬¦ {start_pos}-{end_pos}")
        
        return boundary_content, start_pos, end_pos

    async def optimize_single_boundary_async(self, session: aiohttp.ClientSession,
                                              boundary_content: str, 
                                              boundary_index: int) -> tuple:
        """
        å¼‚æ­¥ä¼˜åŒ–å•ä¸ªè¾¹ç•Œ
        
        Returns:
            (è¾¹ç•Œç´¢å¼•, ä¿®å¤åçš„å†…å®¹, æ˜¯å¦æˆåŠŸ)
        """
        self.logger.info(f"ğŸ”§ æ­£åœ¨ä¼˜åŒ–è¾¹ç•Œ {boundary_index + 1}...")
        self.logger.debug(f"ğŸ“¥ è¾¹ç•Œ {boundary_index + 1} è¾“å…¥å†…å®¹:\n{'='*60}\n{boundary_content}\n{'='*60}")
        
        for attempt in range(self.config['retry_attempts']):
            try:
                prompt = self.get_boundary_optimization_prompt().format(
                    boundary_content=boundary_content
                )
                
                payload = {
                    "model": self.config['model'],
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘åŠ©æ‰‹, ç¿»è¯‘ä¸“å®¶ï¼Œä¸“æ³¨äºä¿®å¤æ–‡æœ¬åˆ†å‰²è¾¹ç•Œé—®é¢˜ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": self.config['temperature'],
                    "max_tokens": 2000
                }
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                timeout = aiohttp.ClientTimeout(total=self.config['request_timeout'])
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=timeout
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        if 'choices' in result and result['choices']:
                            fixed_content = result['choices'][0]['message']['content'].strip()
                            self.logger.info(f"âœ… è¾¹ç•Œ {boundary_index + 1} ä¼˜åŒ–å®Œæˆ")
                            self.logger.debug(f"ğŸ“¤ è¾¹ç•Œ {boundary_index + 1} AIè¿”å›å†…å®¹:\n{'='*60}\n{fixed_content}\n{'='*60}")
                            return (boundary_index, fixed_content, True)
                        else:
                            raise Exception("APIå“åº”æ ¼å¼é”™è¯¯")
                    else:
                        error_text = await response.text()
                        raise Exception(f"APIé”™è¯¯ {response.status}: {error_text}")
                        
            except Exception as e:
                wait_time = self.config['retry_delay'] * (2 ** attempt)
                self.logger.warning(f"è¾¹ç•Œ {boundary_index + 1} ä¼˜åŒ–å¤±è´¥ (ç¬¬{attempt+1}æ¬¡): {e}")
                
                if attempt < self.config['retry_attempts'] - 1:
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"è¾¹ç•Œ {boundary_index + 1} ä¼˜åŒ–å½»åº•å¤±è´¥")
                    return (boundary_index, None, False)

    def apply_boundary_fix(self, content: str, start_pos: int, end_pos: int,
                           fixed_content: str) -> str:
        """åº”ç”¨å•ä¸ªè¾¹ç•Œä¿®å¤"""
        if not fixed_content:
            return content
        
        # è®°å½•è¢«æ›¿æ¢çš„åŸå§‹å†…å®¹
        original_segment = content[start_pos:end_pos]
        self.logger.debug(f"ğŸ”„ è¾¹ç•Œä¿®å¤ - åŸå§‹å†…å®¹ (ä½ç½® {start_pos}-{end_pos}):\n{'='*60}\n{original_segment}\n{'='*60}")
        self.logger.debug(f"ğŸ”„ è¾¹ç•Œä¿®å¤ - æ›¿æ¢ä¸º:\n{'='*60}\n{fixed_content}\n{'='*60}")
        
        # æ¸…ç†ä¿®å¤å†…å®¹
        cleaned_fix = fixed_content.strip()
        # ç§»é™¤å¯èƒ½çš„æ ‡è®°ï¼ˆå¦‚æœAIä»ç„¶è¿”å›äº†çš„è¯ï¼‰
        cleaned_fix = cleaned_fix.replace('---SPLIT_POINT---', '\n\n')
        cleaned_fix = re.sub(r'\n{3,}', '\n\n', cleaned_fix)
        cleaned_fix = cleaned_fix.strip()
        
        # æ›¿æ¢åŸå§‹å†…å®¹
        new_content = content[:start_pos] + cleaned_fix + content[end_pos:]
        
        return new_content

    async def optimize_boundaries_async(self, content: str, 
                                         split_positions: List[int]) -> str:
        """
        å¼‚æ­¥ä¼˜åŒ–æ‰€æœ‰è¾¹ç•Œ
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            split_positions: åˆ†å‰²ç‚¹ä½ç½®åˆ—è¡¨
            
        Returns:
            ä¼˜åŒ–åçš„å†…å®¹
        """
        if not split_positions:
            self.logger.info("âœ… æ²¡æœ‰éœ€è¦ä¼˜åŒ–çš„è¾¹ç•Œ")
            return content
        
        self.logger.info(f"ğŸ”§ å¼€å§‹è¾¹ç•Œä¼˜åŒ–: {len(split_positions)} ä¸ªåˆ†å‰²ç‚¹")
        
        # é¡ºåºå¤„ç†æ¯ä¸ªè¾¹ç•Œï¼Œå¹¶å®æ—¶æ›´æ–°ä½ç½®
        connector = aiohttp.TCPConnector(limit=1, limit_per_host=1)
        
        # ä»åå‘å‰å¤„ç†ï¼Œè¿™æ ·å‰é¢çš„ä½ç½®ä¸ä¼šå—å½±å“
        sorted_positions = sorted(enumerate(split_positions), key=lambda x: x[1], reverse=True)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            for original_idx, split_pos in sorted_positions:
                # æå–å½“å‰è¾¹ç•Œä¸Šä¸‹æ–‡
                boundary_content, start_pos, end_pos = self.extract_boundary_context(
                    content, split_pos
                )
                
                # ä¼˜åŒ–è¾¹ç•Œ
                idx, fixed_content, success = await self.optimize_single_boundary_async(
                    session, boundary_content, original_idx
                )
                
                # åº”ç”¨ä¿®å¤
                if success and fixed_content:
                    content = self.apply_boundary_fix(
                        content, start_pos, end_pos, fixed_content
                    )
                    self.logger.debug(f"åº”ç”¨è¾¹ç•Œ {original_idx + 1} ä¿®å¤")
        
        self.logger.info(f"ğŸ‰ è¾¹ç•Œä¼˜åŒ–å®Œæˆ! æˆåŠŸä¼˜åŒ– {len(split_positions)} å¤„è¾¹ç•Œ")
        
        return content

    def process_file(self, input_file: str, output_file: str = None):
        """åŒæ­¥æ¥å£"""
        return asyncio.run(self.process_file_async(input_file, output_file))

    def batch_process_folder(self, input_folder: str, output_folder: str = None, file_pattern: str = "*.txt"):
        """æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        input_path = Path(input_folder)
        if not input_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹: {input_folder}")
        
        if not input_path.is_dir():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {input_folder}")
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹
        if output_folder is None:
            output_folder = input_path.parent / "processed"
        output_path = Path(output_folder)
        output_path.mkdir(exist_ok=True)
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        pattern_path = input_path / file_pattern
        files = list(input_path.glob(file_pattern))
        
        if not files:
            self.logger.warning(f"åœ¨æ–‡ä»¶å¤¹ {input_folder} ä¸­æœªæ‰¾åˆ°åŒ¹é… {file_pattern} çš„æ–‡ä»¶")
            return []
        
        self.logger.info(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        processed_files = []
        total_start_time = time.time()
        
        for i, input_file in enumerate(files, 1):
            try:
                self.logger.info(f"\n{'='*50}")
                self.logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {input_file.name}")
                self.logger.info(f"{'='*50}")
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆä¿ç•™åŸå§‹æ–‡ä»¶åï¼Œæ ‡é¢˜æå–ç”± formatter å¤„ç†ï¼‰
                output_file = output_path / f"{input_file.stem}.md"
                
                # å¤„ç†æ–‡ä»¶
                result_file = self.process_file(str(input_file), str(output_file))
                processed_files.append({
                    'input': str(input_file),
                    'output': result_file,
                    'status': 'success'
                })
                
                self.logger.info(f"âœ… å®Œæˆ: {input_file.name} -> {Path(result_file).name}")
                
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥ {input_file.name}: {e}")
                processed_files.append({
                    'input': str(input_file),
                    'output': None,
                    'status': 'failed',
                    'error': str(e)
                })
        
        total_elapsed = time.time() - total_start_time
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for f in processed_files if f['status'] == 'success')
        failed_count = len(processed_files) - success_count
        
        self.logger.info(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        self.logger.info(f"ğŸ“Š æ€»è®¡: {len(processed_files)} ä¸ªæ–‡ä»¶")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
        self.logger.info(f"âŒ å¤±è´¥: {failed_count} ä¸ª")
        self.logger.info(f"â±ï¸  æ€»ç”¨æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ")
        self.logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
        
        return processed_files

    # def clean_filename(self, filename: str) -> str:
    #     """æ¸…ç†æ–‡ä»¶åï¼Œç”Ÿæˆæ›´ç®€æ´çš„è¾“å‡ºæ–‡ä»¶å"""
    #     clean = re.sub(r'[\[\](){}]', '', filename)
    #     clean = re.sub(r'[^\w\s-]', '', clean)
    #     clean = re.sub(r'\s+', '_', clean.strip())
    #     clean = clean.replace('__', '_').strip('_')
        
    #     # é™åˆ¶é•¿åº¦
    #     if len(clean) > 50:
    #         clean = clean[:50].rstrip('_')
        
    #     return clean or "processed_file"

def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–å­—å¹•è½¬æ¢å™¨ - é¡ºåºå¤„ç†ã€æ™ºèƒ½åˆ†å—ã€æ®µè½çº§ç¿»è¯‘')
    parser.add_argument('--input_path', help='è¾“å…¥å­—å¹•æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„', default='../raw')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„', default="../output")
    parser.add_argument('-k', '--api-key', help='DeepSeek APIå¯†é’¥')
    parser.add_argument('--chunk-size', type=int, help='æ¯å—åŒ…å«çš„å•è¯æ•°', default=1200)
    parser.add_argument('--temperature', type=float, default=0.1, help='AIæ¸©åº¦å‚æ•°')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼ï¼Œå¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶')
    parser.add_argument('--pattern', default='*.txt', help='æ‰¹é‡æ¨¡å¼ä¸‹çš„æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *.txt)')
    parser.add_argument('--enable-boundary-optimization', action='store_true', default=True,
                        help='å¯ç”¨è¾¹ç•Œä¼˜åŒ–ï¼ˆå¤„ç†å®Œæˆåç”¨AIä¼˜åŒ–åˆ†å—åˆ†å‰²å¤„ï¼Œé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--disable-boundary-optimization', action='store_true',
                        help='ç¦ç”¨è¾¹ç•Œä¼˜åŒ–')
    
    args = parser.parse_args()
    
    # è·å–APIå¯†é’¥
    api_key = args.api_key or os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: è¯·æä¾›DeepSeek APIå¯†é’¥")
        print("ğŸ’¡ æ–¹æ³•1: ä½¿ç”¨ -k å‚æ•°")
        print("ğŸ’¡ æ–¹æ³•2: è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")
        return
    
    # é…ç½®
    config = {
        "chunk_size": args.chunk_size,
        "temperature": args.temperature,
        "enable_boundary_optimization": args.enable_boundary_optimization and not args.disable_boundary_optimization
    }
    
    try:
        converter = OptimizedSubtitleConverter(api_key, config)
        
        # åˆ¤æ–­æ˜¯æ‰¹é‡å¤„ç†è¿˜æ˜¯å•æ–‡ä»¶å¤„ç†
        input_path = Path(args.input_path)
    
        print("ğŸš€ å¯åŠ¨æ‰¹é‡å¤„ç†æ¨¡å¼")
        if not input_path.is_dir():
            print(f"âŒ é”™è¯¯: æ‰¹é‡æ¨¡å¼éœ€è¦æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä½†æä¾›çš„æ˜¯æ–‡ä»¶: {args.input_path}")
            return
        
        results = converter.batch_process_folder(
            str(input_path), 
            args.output
        )
        
        success_count = sum(1 for r in results if r['status'] == 'success')
        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸå¤„ç† {success_count}/{len(results)} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        logging.error(f"å¤„ç†å¤±è´¥: {e}", exc_info=True)


if __name__ == "__main__":
    main()